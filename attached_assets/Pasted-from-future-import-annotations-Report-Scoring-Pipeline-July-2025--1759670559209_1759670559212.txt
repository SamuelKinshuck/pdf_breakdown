from __future__ import annotations
"""
Report‑Scoring Pipeline · **(July 2025)**
------------------------------------------------

* **composite_scores** now returns the raw exponential‑decay vector and the
  final semantic‑adjusted weight vector for every page/layer, alongside the
  three overall layer scores.
* **process_pdf** converts those vectors into explicit columns on the returned
  ``pandas.DataFrame`` (one row per page plus a summary row), so that external
  consumers can easily verify the weighted‑average maths.
* The entry‑point logic now concatenates those per‑PDF dataframes instead of
  extending a list of dicts.

"""

import json
import math
import os
import re
import sys
import tempfile
from glob import glob
from pathlib import Path
from typing import Dict, List, Tuple
from PIL import Image

import fitz  # PyMuPDF
from fitz import Document
import numpy as np
import pandas as pd
from tqdm import tqdm

from gpt_interface import (
    get_response_from_chatgpt_multiple_image_and_functions,
    get_response_from_chatgpt_multiple_image,
    get_response_from_chatgpt_simple,
    get_response_from_chatgpt_with_functions
)
from jobstate import JobState, Message
from datetime import datetime

# Creating a logger doc
from log_config import get_logger, shutdown_logging
logger = get_logger (__name__)

from time_poor_executive import simulate_executive_reading
from params import *
from dsl import p as p_tag, h, ul, kv, progress, modal
from params import FIRST_PAGE_WINDOW, LAST_PAGE_WINDOW

if os.path.exists(CSV_OUT):
    already_outputted = pd.read_csv(CSV_OUT)
    pdf_already_done = already_outputted['pdf'].unique()
else:
    pdf_already_done = []
# behavioural constants
DECAY = {1 : 1, 60: 0.05, 600: 0.02, 6000: 0.01} #1 means time-poor; no decay for them.

DEFAULT_MIDDLE_WEIGHT = 0.10
DEFAULT_FIRST_PAGES_WEIGHT = 0.15
BOILERPLATE_WEIGHT = 0.01
BOOST_600_OUTSIDE_FIRST_PAGES = 2

# ── Prompt templates (unchanged) ─────────────────────────────────────────────
# [snip: identical prompts from previous version]
from prompts import (  # local helper that simply re‑exports unchanged strings
    START_PAGES,
    SUMMARY_PROMPT,
    SKIM_SYSTEM,
    SKIM_USER_TMPL,
    SKIM_USER_IMAGE_PROMPT,
    BOILER_SYSTEM,
    BOILER_USER_TMPL,
    BOILER_USER_IMAGE_PROMPT,
    PAGE_SCORE_SYSTEM,
    PAGE_SCORE_USER_TMPL,
    MEMO_SYSTEM,
    get_skim_schema,
    get_boiler_schema,
    get_batch_schema
)

# ── Helper functions ─────────────────────────────────────────────────────────

def ensure_image_size(path: Path,
                      max_bytes: int = 10 * 1024 * 1024,
                      min_quality: int = 30,
                      downscale_step: float = 0.90) -> Path:
    """
    Guarantee that *path* is ≤ *max_bytes* (default 10 MB).

    • First re‑encodes as JPEG, stepping quality ↓ by 5 each loop.
    • If that fails, scales the image down 10 % per loop until the file
      fits or the shortest edge drops below 256 px.
    • Works in‑place; returns the same Path for convenience.
    """
    if path.stat().st_size <= max_bytes:
        return path                       # already small enough

    with Image.open(path) as im:
        # Always work in RGB; PNG transparency adds bytes.
        if im.mode in ("RGBA", "P"):
            im = im.convert("RGB")

        # 1️⃣ quality sweep
        for q in range(85, min_quality - 1, -5):
            im.save(path, format="JPEG",
                     quality=q, optimize=True, progressive=True)
            if path.stat().st_size <= max_bytes:
                return path

        # 2️⃣ resolution sweep
        while path.stat().st_size > max_bytes and min(im.size) > 256:
            new_w = int(im.width * downscale_step)
            new_h = int(im.height * downscale_step)
            im = im.resize((new_w, new_h), Image.LANCZOS)
            im.save(path, format="JPEG",
                     quality=min_quality, optimize=True, progressive=True)

    # If we get here the file is still > max_bytes but we’ve done our best.
    return path


def _json_or_empty(txt: str) -> Dict:
    try:
        return json.loads(txt)
    except json.JSONDecodeError:
        return {}


def extract_text_snippets(doc: List, limit: int = TEXT_CHARS) -> List[str]:
    snippets, used = [], 0
    for page in doc:
        txt = page.get_text("text")
        snippets.append(txt)
        used += len(txt)
        if used >= limit:
            break
    return snippets


def pdf_pages_to_images(pdf_path: Path, dpi: int = DPI) -> List[Path]:
    doc = fitz.open(pdf_path)
    scale = dpi / 72
    mtx = fitz.Matrix(scale, scale)
    tmp = Path(tempfile.mkdtemp(prefix=f"{pdf_path.stem}_"))
    paths: List[Path] = []
    for i, page in enumerate(doc):
        if TEST:
            if i >= TEST_PAGES:
                break
        pix = page.get_pixmap(matrix=mtx)
        out = tmp / f"page_{i:04d}.png"
        pix.save(out)
        ensure_image_size(out)
        paths.append(out)
    doc.close()
    return paths

# ── Weight logic ─────────────────────────────────────────────────────────────

def _doc_has_text(doc: Document) -> bool:
    return any(page.get_text("text").strip() for page in doc.pages())


def build_weights(doc: Document, img_paths: List[Path], this_pdf: str, total_pages_in_practice: int) -> Tuple[List[float], List[int]]:
    """Return (semantic_weight_list, boilerplate_flag_list). 0‑based indexing."""

    sem = [DEFAULT_MIDDLE_WEIGHT] * total_pages_in_practice
    boil = [0] * total_pages_in_practice

    is_textual = _doc_has_text(doc)
    first_pages = list(range(0, min(FIRST_PAGE_WINDOW, total_pages_in_practice)))
    schema = get_skim_schema(first_pages)

    # 1) Skim likelihood → semantic weight first pages
    logger.info('*' * 20)
    logger.info(f"Getting skim likelihood for pages of {this_pdf}")
    if is_textual:
        payload = [{
            "page": i+1, #whenever we talk to chatgpt, we translate from 0-indexing to 1-indexing
            "text": doc[i].get_text("text")[:800]
            } for i in first_pages]

        resp = get_response_from_chatgpt_with_functions(
            system_prompt=SKIM_SYSTEM,
            user_prompt=SKIM_USER_TMPL % json.dumps(payload, ensure_ascii=False),
            model=MODEL,
            functions = schema,
            function_name = 'assess_skim_likelihood',
            temperature = 0
        )
    else:
        imgs = [str(img_paths[i]) for i in first_pages]
        resp = get_response_from_chatgpt_multiple_image_and_functions(
            system_prompt=SKIM_SYSTEM,
            user_prompt=SKIM_USER_IMAGE_PROMPT,
            image_paths=imgs,
            model=MODEL,
            functions = schema,
            function_name = 'assess_skim_likelihood'
        )
    skim_json = _json_or_empty(resp)
    logger.info('Determined skim likelihood as follows (using 1-indexing): ')
    logger.debug(skim_json)
    if skim_json:
        for p, w in skim_json.items():
            #when we process a response from chatGPT, translate back to 0-indexed page numbers
            sem[int(p)-1] = float(w)
    else:
        print("GPT failed to get skim likelihoods")
        raise ValueError("GPT failed to get skim likelihoods")

        # 2) Boilerplate flags on last pages
    last_pages_start = max(0, total_pages_in_practice - LAST_PAGE_WINDOW)
    last_pages = list(range(last_pages_start, total_pages_in_practice))
    
    if TEST:
        return sem, boil

    logger.info('*' * 20)
    logger.info('Determining boilerplate status for final pages')
    if is_textual:
        payload2 = [
            {
                "page": i+1, #whenever we talk to chatgpt, we translate from 0-indexing to 1-indexing
                "text": doc[i].get_text("text")[:800]
            } for i in last_pages]
        boil_resp = get_response_from_chatgpt_with_functions(
            system_prompt=BOILER_SYSTEM,
            user_prompt=BOILER_USER_TMPL % json.dumps(payload2, ensure_ascii=False),
            model=MODEL,
            functions = get_boiler_schema([j for j in last_pages]),
            function_name = 'assess_boilerplate_status',
            temperature = 0
        )
    else:
        imgs = [str(img_paths[j]) for j in last_pages]
        last_pages_str = '; '.join([str(j+1) for j in last_pages]) #anything said to chatgpt needs to be expressed in 1-indexing language
        boil_resp = get_response_from_chatgpt_multiple_image_and_functions(
            system_prompt=BOILER_SYSTEM,
            user_prompt=BOILER_USER_IMAGE_PROMPT + f'\n\n\nNote that the numbers of the pages you see are as follows :{last_pages_str}',
            image_paths=imgs,
            model=MODEL,
            functions = get_boiler_schema([j for j in last_pages]),
            function_name = 'assess_boilerplate_status'
        )

    boil_json = _json_or_empty(boil_resp)
    
    logger.info('boilerplate status determined as follows:')
    logger.debug(boil_json)
    if boil_json:
        for p_str, flag in boil_json.items():
            p = int(p_str) - 1 #when we process a response from ChatGPT, we translate back to 0 indexing
            if p not in [j for j in last_pages]:
                print('!' * 80)
                print("ChatGPT rated boilerplate status of the wrong page")
                continue
            if flag == 1:
                sem[p] = BOILERPLATE_WEIGHT
                boil[p] = 1

    # 3) Ensure early‑page dominance
    if sum(sem[i] for i in first_pages) <= sum(sem[i] for i in range(FIRST_PAGE_WINDOW, total_pages_in_practice)):
        factor = 1.1 * (
            sum(sem[i] for i in range(1, total_pages_in_practice)) / max(sum(sem[i] for i in first_pages), 0.01)
        )
        for i in first_pages:
            sem[i] *= factor

    return sem, boil

# ── Scoring maths ───────────────────────────────────────────────────────────

def _layer_weight(page: int, layer: int, sem: List[float], boil: List[int], time_poor_weights: Dict[int, float]) -> float:
    if layer == 1:
        return time_poor_weights.get(page, 0)
    sem_w = sem[page] if layer != 6000 else (1.0 if not boil[page] else BOILERPLATE_WEIGHT)
    if layer == 600 and page >= FIRST_PAGE_WINDOW:
        sem_w *= BOOST_600_OUTSIDE_FIRST_PAGES
    return sem_w * math.exp(-DECAY[layer] * page)


def composite_scores(
    pages: List[int],
    metrics: Dict[int, Dict[str, float]],
    sem: List[float],
    boil: List[int],
    time_poor_weights: Dict[int, float]
) -> Tuple[
    float,  # s60
    float,  # s600
    float,  # s6000
    float,
    Dict[int, List[float]],  # decay vectors per layer
    Dict[int, List[float]],  # final weight vectors per layer
]:
    """Return overall scores *and* the per‑page decay and final weight vectors.

    decay_vectors[layer][i]  ==  exp(‑DECAY[layer] * page_i)
    weight_vectors[layer][i] ==  final semantic‑adjusted weight for page_i/layer
    (page_i corresponds to ``pages[i]``).
    """

    decay_vectors: Dict[int, List[float]] = {60: [], 600: [], 6000: [], 1 : []}
    weight_vectors: Dict[int, List[float]] = {60: [], 600: [], 6000: [], 1 : []}
    results: Dict[int, float] = {}

    for layer in (60, 600, 6000, 1): #1 corresponds to 'time_poor'
        numer = {m: 0.0 for m in ("beauty", "neatness", "ease")}
        denom = 0.0
        for p in pages:
            decay = math.exp(-DECAY[layer] * p)
            w = _layer_weight(p, layer, sem, boil, time_poor_weights)

            decay_vectors[layer].append(decay)
            weight_vectors[layer].append(w)

            denom += w
            for m in numer:
                numer[m] += w * metrics[p][m]

        with np.errstate(invalid="ignore"):
            layer_comp = sum(
                METRIC_RATIOS[layer][m] * (numer[m] / denom) if denom else 0.0 for m in numer
            )
        results[layer] = layer_comp

    return (
        results[60],
        results[600],
        results[6000],
        results[1],
        decay_vectors,
        weight_vectors,
    )



def parse_flat_metrics(payload: Dict) -> Tuple[Dict[int, Dict[str, float]], str]:
    page_metrics: Dict[int, Dict[str, float]] = {}
    updated_profile = payload.get("updated_profile", "")

    for k, v in payload.items():
        m = re.match(r"^(beauty|neatness|ease|comment)_(\d+)$", k)
        if not m:
            continue
        metric, page_s = m.groups()
        page = int(page_s)
        if page not in page_metrics:
            page_metrics[page] = {"beauty": np.nan, "neatness": np.nan, "ease": np.nan, "comment": "no commet"}
        if metric == "comment":
            val = str(v).strip() or "no commet"
            page_metrics[page]["comment"] = val
        else:
            try:
                page_metrics[page][metric] = float(v)
            except (TypeError, ValueError):
                page_metrics[page][metric] = np.nan
    return page_metrics, updated_profile.strip()


# ── Fast‑path for slim documents (≤FIRST_PAGE_WINDOW pages) ────────────────────────────────
def process_short_pdf(pdf: Path, job: JobState) -> pd.DataFrame:
    """
    Evaluate a short report in one shot and return an OVERALL row that is
    schema‑compatible with the output of `process_pdf`.

    The heavy per‑page logic is skipped; GPT‑4 is asked to supply:
      • composite scores (s60, s600, s6000, s_time_poor)
      • an estimated elapsed_time_poor and executive skim summary
      • the editorial feedback memo

    Returns
    -------
    pd.DataFrame
        Single row with `page == "OVERALL"`.
    """

    # Render every page once
    img_paths = pdf_pages_to_images(pdf)
    n_pages = len(img_paths)

    # ── 1. Ask GPT‑4 to do the whole rating in one message ────────────────
    SHORT_REPORT_SYSTEM = (
        "You are the **Short‑Report-Appearance-Rater** in an automated PDF‑quality "
        f"pipeline. You will see ≤{n_pages} consecutive page images. "
        "Your task is to score them. "
        "**Scoring System** "
        "***s60*** "
        "How good does it look from the perspective of someone who only has 60 seconds to read? "
        "This person is particularly influenced by the beauty of the report, and the ability to extract "
        "Tidbits of substantive information really quickly. They are also concerned about neatness."
        "\n"
        "***s600*** "
        "How good does it look from the perspective of someone who only has 600 seconds to read? "
        "This person is also influenced by the beauty of the report, but to a slightly lesser extent."
        "They have time to process it more comprehensively, so being able to pick things up immediately "
        "is a bit less of a concern. However, they will still be put off if the content is messy or visually unappealing."
        "\n"
        "***s6000*** "
        "How good does it look from the perspective of someone who only has 6000 seconds to read? "
        "This hypothetical person is willing to process the report comprehensively to understand its arguments "
        "in depth. They still want it to look nice and neat, and they will have an easier job if information is presented"
        "in an understandable way. But they may be less prone to simply ignoring things that they cannot grasp immediately."

        "All the above scores should be out of 10 (0 means rubbish, 10 amazing). "
        "Be very, very strict in your judgements. Only an absolute best in class exemplar of stunning performance in all categories should merit " 
        "anything near the top of the range. "
        "Even an 8 would be considered excellent, so the typical report would get a 2 or 3 on most metrics "

        "**Memo**"
        "Memo is where you provide comments on the report's appearance, focusing on what works, what does not, and how to improve."
    )

    schema = [
        {
            "type": "function",
            "function": {
                "name": "score_short_pdf",
                "description": "Overall quality scores and synthesis for a short report",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "s60": {"type": "number"},
                        "s600": {"type": "number"},
                        "s6000": {"type": "number"},
                        "memo": {"type": "string"},
                    },
                    "required": [
                        "s60",
                        "s600",
                        "s6000",
                        "memo",
                    ],
                },
            },
        }
    ]

    pdf_friendly_name = pdf.name

    job.messages.append(
        Message(
            time = datetime.now(),
            node= p_tag(f'Rating pdf {pdf_friendly_name}, which is short enough to do all in one go.'),
            level = 'INFO'
        )
    )
    raw = get_response_from_chatgpt_multiple_image_and_functions(
        system_prompt=SHORT_REPORT_SYSTEM,
        user_prompt="Evaluate the entire report shown in these images.",
        image_paths=[str(p) for p in img_paths],
        model=MODEL,
        functions=schema,
        function_name="score_short_pdf",
    )

    parsed = _json_or_empty(raw)
    if not parsed:
        raise RuntimeError(f"GPT did not return JSON for {pdf.name}")

    memo = parsed["memo"].strip()
    job.messages.append(
        Message(
            time = datetime.now(),
            node= ul([h(f'Got GPT to rate {pdf_friendly_name}. The comments were:', 3), p_tag(f'{memo}')]),
            level = 'INFO'
        )
    )
    # ── 2. Build the single OVERALL row ────────────────────────────────────
    row = {
        "pdf": pdf.name,
        "page": "OVERALL",
        "beauty": None,
        "neatness": None,
        "ease": None,
        "comment": memo,
        "sem": None,
        "boil": None,
        "decay60": None,
        "decay600": None,
        "decay6000": None,
        "weight60": None,
        "weight600": None,
        "weight6000": None,
        "weight_time_poor": None,
        "s60": round(float(parsed["s60"]), 2),
        "s600": round(float(parsed["s600"]), 2),
        "s6000": round(float(parsed["s6000"]), 2),
        "s_time_poor": None,
        "elapsed_time_poor": None,
        "summary_time_poor": None,
    }

    return pd.DataFrame([row])


# ── Core processing per PDF ─────────────────────────────────────────────────
def process_pdf(pdf: Path, job: JobState) -> pd.DataFrame:

    doc = fitz.open(pdf)
    job.messages.append(
        Message(
            time = datetime.now(),
            node= p_tag(f'Opened pdf {pdf.name}'),
            level = 'INFO'
        )
    )

    try:
        total_pages = len(doc)
        total_pages_in_practice = min(total_pages, TEST_PAGES) if TEST else total_pages
        page_idxs = range(total_pages_in_practice)

        job.messages.append(
            Message(
                time = datetime.now(),
                node = p_tag(f'Pdf {pdf.name} has {total_pages_in_practice} pages that will be iterated through'),
                level = 'INFO'
            )
        )

        # Work with indices, not page objects
        # Example: text snippets
        snippets = [doc.load_page(i) for i in page_idxs]

        if total_pages <= FIRST_PAGE_WINDOW:
            return process_short_pdf(pdf, job)

        # 1️⃣ images & summary --------------------------------------------------
        img_paths = pdf_pages_to_images(pdf)
        context_txt = "".join(extract_text_snippets(snippets))

        logger.info('*' * 20)
        logger.info(f"Summarising the content of {str(pdf)}")
        if context_txt.strip():
            summary = get_response_from_chatgpt_simple(
                system_prompt="You are an expert summariser. Summarise the content in 200 words.",
                user_prompt=f"{SUMMARY_PROMPT}\n\nTEXT:\n{context_txt}",
                model=MODEL,
            )
            job.messages.append(
                Message(
                    time = datetime.now(),
                    node = modal(
                        short_message = f'Before iterating through {pdf.name}, we summarise it based on the text of the first {FIRST_PAGE_WINDOW} pages. Click to see the summary',
                        long_message = f'{summary}'
                    ),
                    level = 'INFO'
                )
            )
        else:
            imgs = [str(img_paths[i]) for i in range(min(START_PAGES, total_pages_in_practice))]
            summary = get_response_from_chatgpt_multiple_image(
                system_prompt="Summarise the content of all the images you see. Summarise the content in 200 words.",
                user_prompt="Here are the images.",
                image_paths=imgs,
                model=MODEL,
            )

            job.messages.append(
                Message(
                    time = datetime.now(),
                    node=modal(
                        short_message=f'Before iterating through {pdf.name}, we summarise it based on images of the first {FIRST_PAGE_WINDOW} pages. Click to see the summary',
                        long_message=f'{summary}'
                    ),
                    level = 'INFO'
                )
            )

        logger.info(f"The summary is: {summary}")

        # 2️⃣ Weights -----------------------------------------------------------
        logger.info('*' * 20)
        logger.info(f'getting standard weights for {str(pdf)}')
        sem, boil = build_weights(doc, img_paths, str(pdf), total_pages_in_practice)
        sem_str = [str(i) for i in sem]
        boil_str = [str(i) for i in boil]
        job.messages.append(
            Message(
                time = datetime.now(),
                node= ul(
                    [
                        h(f'Got page-weights for {pdf.name}', 6) ,
                        kv([
                            ('Meaningful page weights:',  f'{"; ".join(sem_str)}'),
                            ('boilerplate flags:', f'{"; ".join(boil_str)}')
                        ])
                    ]
                ),
                level = 'INFO'
            )
        )
        logger.info('*' * 20)
        logger.info(f'getting time poor weights for {str(pdf)}')
        time_budget = 60 if not TEST else 10
        weights_time_poor, elapsed, exec_summary = simulate_executive_reading(pdf, img_paths, total_pages_in_practice, job, time_budget)
        job.messages.append(
            Message(
                time = datetime.now(),
                node= modal(
                    short_message = f'Parsed through {pdf.name} from the perspective of a time-poor executive. The executive read for {elapsed} seconds. Click to see what information they gleaned',
                    long_message = f'{exec_summary}'
                ),
                level = 'INFO'
            )
        )

        # trackers -------------------------------------------------------------
        page_metrics: Dict[int, Dict[str, float]] = {i: {"beauty": np.nan, "neatness": np.nan, "ease": np.nan, "comment": "no comment"} for i in range(0, total_pages_in_practice)}
        rolling_profile = ""
        rolling_profile_raw = ""

        logger.info('*' * 40)
        logger.info("Rating pages on metrics")
        for start in tqdm(range(0, total_pages_in_practice, BATCH_SIZE), unit="batch"):
            pages = list(range(start, min(start + BATCH_SIZE, total_pages_in_practice)))
            imgs = [str(img_paths[p]) for p in pages]

            pages_1_indexed = [p+1 for p in pages] #translate to 1-indexed page numbers when talking to chatgpt.
            pages_1_indexed_str = map(str, pages_1_indexed)
            user_prompt = (
                f"You are seeing pages: {', '.join(pages_1_indexed_str)}\nOverall summary:\n{summary}\n\nRolling profile so far:\n{rolling_profile or '<none>'}"
            )
            schema = get_batch_schema(pages)
            job.messages.append(
                Message(
                    time = datetime.now(),
                    node = progress(current = start, total = total_pages_in_practice),
                    level = 'INFO'
                )
            )
            raw = get_response_from_chatgpt_multiple_image_and_functions(
                system_prompt=PAGE_SCORE_SYSTEM,
                user_prompt=user_prompt + "\n" + PAGE_SCORE_USER_TMPL,
                image_paths=imgs,
                model=MODEL,
                functions=schema,
                function_name="score_pages",
            )
            parsed = _json_or_empty(raw)
            metrics_batch, profile_delta = parse_flat_metrics(parsed)
            for p, vals in metrics_batch.items():
                page_metrics[p-1].update(vals) #convert back to 0-indexing when processing responses from chatgpt

            logger.info('*' * 20)
            logger.info(f'Ratings obtained for {str(pdf)}, for the following pages: {", ".join(map(str, pages))}')
            logger.debug(parsed)
            logger.info("Profile delta: ")
            logger.debug(profile_delta)

            rolling_profile += "\n" + profile_delta
            rolling_profile_raw += "\n" + profile_delta
            if len(rolling_profile) > ROLLING_MAX_CHARS:
                rolling_profile = get_response_from_chatgpt_simple(
                    system_prompt="You are a brilliant summariser. Condense to ≤1000 chars.",
                    user_prompt=rolling_profile,
                    model=MODEL,
                )

            job.messages.append(
                Message(
                    time = datetime.now(),
                    node= modal(
                        short_message = f'Rated pages {"; ".join(map(str, pages_1_indexed))} for pdf {pdf.name}. Click to see the "rolling profile"',
                        long_message = f'{rolling_profile}'
                    ),
                    level = 'INFO'
                )
            )
        # 3️⃣ Overall layer composites ----------------------------------------
        logger.info(f"Getting overall scores for {str(pdf)}")
        all_pages = list(range(0, total_pages_in_practice))
        s60, s600, s6k, s_time_poor, decay_vecs, weight_vecs = composite_scores(all_pages, page_metrics, sem, boil, weights_time_poor)

        # 4️⃣ Memo -------------------------------------------------------------
        memo = get_response_from_chatgpt_simple(
            system_prompt=MEMO_SYSTEM,
            user_prompt=f"Content to summary:\n\n{rolling_profile_raw}",
            model=MODEL,
        )
        logger.info(f"Got Memo for {str(pdf)}")
        logger.debug(memo)

        job.messages.append(
            Message(
                time = datetime.now(),
                node= modal(
                    short_message = f'Rated all pages for pdf {pdf.name}. Click to see summary of comments',
                    long_message = f'{memo}'
                ),
                level = 'INFO'
            )
        )

        # Build per‑page dataframe -------------------------------------------
        rows: List[dict] = []
        for p in all_pages:
            rows.append(
                {
                    "pdf": pdf.name,
                    "page": p,
                    "beauty": page_metrics[p]["beauty"],
                    "neatness": page_metrics[p]["neatness"],
                    "ease": page_metrics[p]["ease"],
                    "comment": page_metrics[p]["comment"],
                    "sem": sem[p],
                    "boil": boil[p],
                    "decay60": decay_vecs[60][p],
                    "decay600": decay_vecs[600][p],
                    "decay6000": decay_vecs[6000][p],
                    "d_time_poor": 1,
                    "weight60": weight_vecs[60][p],
                    "weight600": weight_vecs[600][p],
                    "weight6000": weight_vecs[6000][p],
                    "weight_time_poor" : weights_time_poor.get(p, 0),
                    "s60": None,
                    "s600": None,
                    "s6000": None,
                    "s_time_poor" : None,
                    "elapsed_time_poor" : None,
                    "summary_time_poor" : None

                }
            )

        # Summary row ---------------------------------------------------------
        rows.append(
            {
                "pdf": pdf.name,
                "page": "OVERALL",
                "beauty": None,
                "neatness": None,
                "ease": None,
                "comment": memo.strip(),
                "decay60": None,
                "decay600": None,
                "decay6000": None,
                "weight60": None,
                "weight600": None,
                "weight6000": None,
                "weight_time_poor" : None,
                "sem" : None,
                "boil" : None,
                "s60": round(s60, 2),
                "s600": round(s600, 2),
                "s6000": round(s6k, 2),
                "s_time_poor" : round(s_time_poor, 2),
                "elapsed_time_poor" : elapsed,
                "summary_time_poor" : exec_summary
                
            }
        )
    finally: 
        doc.close()

    return pd.DataFrame(rows)

# ── Entrypoint ──────────────────────────────────────────────────────────────
if __name__ == '__main__':
    _JOBS : Dict[str, JobState] = {}
    pdf_files = sorted(glob(os.path.join(INPUT_DIR, "*.pdf")))
    if not pdf_files:
        logger.warning(f"No PDF files found in {INPUT_DIR}")

    results_df_list: List[pd.DataFrame] = []

    try:
        for pdf in pdf_files:
            if pdf not in pdf_already_done:
                job = JobState(job_id=pdf, status="PENDING", input_folder=INPUT_DIR, csv_name=CSV_OUT, created_at = datetime.now())
                _JOBS[pdf] = job
                results_df_list.append(process_pdf(Path(pdf), job))
            else:
                job = JobState(job_id=pdf, status="ALREADY_DONE", input_folder=INPUT_DIR, csv_name=CSV_OUT, created_at = datetime.now())
                _JOBS[pdf] = job
    except KeyboardInterrupt:
        logger.info("Aborted by user")
        raise


    # Persist outputs ------------------------------------------------------
    if results_df_list:
        results_df = pd.concat(results_df_list, ignore_index=True)
        if os.path.exists(CSV_OUT):
            results_df = pd.concat([results_df, already_outputted])
        for layer, layer_dict in METRIC_RATIOS.items():
            for metric, metric_weight in layer_dict.items():
                results_df[str(layer) + '_' + str(metric) + '_weight'] = metric_weight
        results_df.to_csv(CSV_OUT, index=False)
        logger.info(f"Page-level results {CSV_OUT}")
    else:
        logger.info("No page‑level data generated – nothing to write.")

    # to remove the logger file
    shutdown_logging()
