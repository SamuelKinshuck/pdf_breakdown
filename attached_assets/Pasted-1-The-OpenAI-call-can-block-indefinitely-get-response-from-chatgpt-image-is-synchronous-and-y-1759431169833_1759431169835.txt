1) The OpenAI call can block indefinitely

get_response_from_chatgpt_image(...) is synchronous and you don’t set a timeout. If that call hangs, your job never progresses and never flips to error.

Fix (add a hard timeout around each page):
Run the OpenAI call in a short-lived thread or concurrent.futures and enforce a timeout; if it times out, record an error for that page and move on.

from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout

EXECUTOR = ThreadPoolExecutor(max_workers=4)
PER_PAGE_TIMEOUT_SECS = 90  # tune to taste

def _call_gpt_with_timeout(**kwargs):
    return get_response_from_chatgpt_image(**kwargs)

# inside _run_process_job loop:
future = EXECUTOR.submit(
    _call_gpt_with_timeout,
    system_prompt=system_prompt,
    user_prompt=user_prompt,
    image_path=None,
    model=model,
    pre_compiled_image=pre_compiled_image
)
try:
    response = future.result(timeout=PER_PAGE_TIMEOUT_SECS)
except FuturesTimeout:
    response = 'Timed out contacting GPT for this page'
except BadRequestError:
    response = 'GPT refused to process this page'
except Exception:
    response = 'Unable to get a response from GPT for this page'


(If your get_response_from_chatgpt_image uses the modern OpenAI Python SDK, you can also set a per-request timeout on the client; the wrapper approach above works regardless.)

2) Image payloads are too big → slow/fragile requests

You’re sending full-page PNGs at 150 DPI, base64-embedded. Some PDFs produce massive rasters; that can swamp the request, hit limits, or just crawl.

Fix (cap size + use JPEG):

Downscale to a max dimension (e.g., 1600 px on the long side)

Convert to JPEG with moderate quality (e.g., 70–80)

Optionally strip alpha

def _images_from_df_path(pdf_path: str, selected_pages: List[int]) -> Dict[int, str]:
    doc = fitz.open(pdf_path)
    page_images_for_gpt = {}
    MAX_DIM = 1600  # px
    for page_num, page in enumerate(doc, start=1):
        if page_num not in selected_pages:
            continue
        # scale to target pixel budget rather than DPI
        rect = page.rect
        scale = min(MAX_DIM / max(rect.width, rect.height), 2)  # clamp overscale
        m = fitz.Matrix(scale, scale)
        pix = page.get_pixmap(matrix=m, alpha=False)
        img_bytes = pix.tobytes("jpeg", quality=75)
        base64_encoded = base64.b64encode(img_bytes).decode('utf-8')
        page_images_for_gpt[page_num] = f"data:image/jpeg;base64,{base64_encoded}"
    return page_images_for_gpt


This dramatically reduces upload size and flakiness.

3) No “stall detector” / watchdog

If the worker freezes, your job stays running forever. Add a watchdog that marks the job as error when there’s no progress for, say, 2–5 minutes. The frontend can surface a “stalled” state.

Fix (lightweight heartbeat check in poll):
Add a last-update timestamp (you already set updated_at in _save_job). In /process_poll, if status == "running" and updated_at is older than N seconds, return status: "error" with a “stalled” message. Or run a background janitor thread that flips stalled jobs.

# inside process_poll(), after building `snap`
STALE_SECS = 180
if snap.get('status') == 'running':
    try:
        from datetime import datetime, timezone
        last = datetime.strptime(snap['updated_at'], "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        if (datetime.now(timezone.utc) - last).total_seconds() > STALE_SECS:
            _save_job(snap['job_id'], {"status": "error", "error": "Worker stalled"})
            # return updated payload
    except Exception:
        pass

4) Occasional malformed/unsupported pages

Some pages (scanned images with huge masks, weird transparency, crazy vector content) can explode rasterization time or memory.

Fix:

Wrap the pixmap step per page with try/except; if it fails or takes too long, skip the page and log a row with the error.

Consider a per-page rasterization timeout too (same future pattern around the pixmap call).

5) SharePoint write can hang (when outputType=sharepoint)

If the job completes the pages but hangs on upload, the poller still shows the last “page done.” You do catch exceptions, but network hangs can block.

Fix:
Wrap sharepoint_export_df_to_csv in a timeout the same way as the OpenAI call, and fail gracefully to browser output as a fallback, or at least mark error.

6) Threading edge cases

You’re using a daemon thread (good), but there’s no limit to concurrent jobs. If lots of jobs start, you can starve threads or run into API rate limits and long retries.

If your OpenAI wrapper has internal retries with large backoffs, it can look like a hang.

Fixes:

Gate concurrency with a ThreadPoolExecutor(max_workers=N) and queue jobs.

Configure the OpenAI client with sane retry/timeout settings (few retries, short total timeout).

7) Make progress visible even when a page is in flight

Right now the UI only moves when a page finishes. Add a “page_started” event so the user sees that page 3 actually began.

Fix (optional):
Before calling GPT, _save_job(job_id, {"pages_done": idx-1, "last_page": page_no, "rows": rows, "page_status": {"page": page_no, "state": "in_progress"}}) and update it after finishing. The frontend can render a spinner on the current page.